{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire Hazard Assessment using Machine Learning and Remote Sensing Data\n",
    "\n",
    "This notebook demonstrates a temporal perspective on wildfire hazard assessment. It includes data preprocessing, several model evaluation experiments using both temporal (balanced) and random undersampling strategies, and visualisation of the results.\n",
    "\n",
    "**Requirements:**\n",
    "- pandas, numpy\n",
    "- matplotlib, seaborn\n",
    "- scikit-learn\n",
    "- imblearn\n",
    "\n",
    "**Note:** Ensure that your DataFrame (`df`) is initialised and contains the required columns. Some variables (e.g. `burned_area`) must be defined beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (roc_auc_score, f1_score, accuracy_score,\n",
    "                             mean_squared_error, make_scorer)\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from matplotlib.ticker import MultipleLocator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Before executing this cell, ensure that you have defined the DataFrame `df` and the variable `burned_area` (for example, as a dictionary mapping years to percentage values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Preprocessing\n",
    "\n",
    "# Rename columns for clarity (adjust the list to match your dataframe)\n",
    "df.columns = [\n",
    "    'index', 'target_fid', '2001', '2002', '2003', '2004',\n",
    "    '2005', '2006', '2007', '2008', '2009', '2010',\n",
    "    '2011', '2012', '2013', '2014', '2015', '2016',\n",
    "    '2017', '2018', '2019', 'Aspect', 'Elevation', \n",
    "    'Land form', 'NDVI', 'Population density', \n",
    "    'Rainfall', 'Windspeed', 'Road density', \n",
    "    'Slope', 'Stream density', 'TWI',\n",
    "    '2023', '2022', '2021', '2020', '2019_2'\n",
    "]\n",
    "\n",
    "# List of year columns to modify (set any value > 0 to 1)\n",
    "columns_to_modify = [\n",
    "    '2001', '2002', '2003', '2004', '2005',\n",
    "    '2006', '2007', '2008', '2009', '2010', \n",
    "    '2011', '2012', '2013', '2014', '2015', \n",
    "    '2016', '2017', '2018', '2019', '2020', \n",
    "    '2021', '2022', '2023'\n",
    "]\n",
    "\n",
    "# Transform values: if cell value > 0, map to 1; otherwise, keep original\n",
    "df[columns_to_modify] = df[columns_to_modify].applymap(lambda x: 1 if x > 0 else x)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cumulative Burned Area Plot\n",
    "# -----------------------------------------------------------------------------\n",
    "# Note: 'burned_area' should be defined (e.g. as a dictionary mapping Year -> Percentage)\n",
    "cumulative_percentage = burned_area\n",
    "\n",
    "# Convert the cumulative percentage data to a DataFrame for plotting\n",
    "cumulative_percentage_df = pd.DataFrame(\n",
    "    list(cumulative_percentage.items()),\n",
    "    columns=['Year', 'Cumulative Percentage']\n",
    ")\n",
    "\n",
    "# Plot a bar plot of the cumulative percentage of the total burned area\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x='Year', y='Cumulative Percentage', data=cumulative_percentage_df, color='grey')\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Cumulative Percentage of Total Burned Area (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation Functions and Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years for splitting the data (adjust as needed)\n",
    "split_years = [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,\n",
    "               2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019,\n",
    "               2020, 2022]\n",
    "\n",
    "# Cross-validation strategy: 10 folds with 2 repeats\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=2, random_state=1)\n",
    "\n",
    "# Define predictor variables (factors) for the models\n",
    "factors = [\n",
    "    'Aspect', 'Elevation', 'Land form', 'NDVI', 'Population density',\n",
    "    'Rainfall', 'Windspeed', 'Road density', 'Slope', 'Stream density', 'TWI'\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3.1 Define Model Dictionaries\n",
    "# -----------------------------------------------------------------------------\n",
    "# Model set for general evaluation (you can modify parameters as required)\n",
    "models = {\n",
    "    'RF': RandomForestClassifier(bootstrap=False, max_depth=6, n_estimators=150, random_state=1),\n",
    "    'HGB': HistGradientBoostingClassifier(learning_rate=0.2, max_iter=200, min_samples_leaf=40),\n",
    "    'MLP': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', MLPClassifier(activation='tanh', hidden_layer_sizes=(12,), max_iter=300, random_state=1))\n",
    "    ]),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=10),\n",
    "    'LR': LogisticRegression(random_state=1)\n",
    "}\n",
    "\n",
    "# Define custom scorers for later use\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'f1': 'f1',\n",
    "    'rmse': make_scorer(mean_squared_error, squared=False)\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function: create_balanced_sample\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_balanced_sample(dataframe, year):\n",
    "    \"\"\"\n",
    "    Create balanced training and testing samples based on a specified year.\n",
    "    \n",
    "    Parameters:\n",
    "      dataframe: Input DataFrame containing a 'Years' column and target 'Wildfire'\n",
    "      year: Year used to split the data\n",
    "      \n",
    "    Returns:\n",
    "      after_year_balanced: Testing DataFrame (samples with 'Years' > year)\n",
    "      before_year_balanced: Training DataFrame (samples with 'Years' < year)\n",
    "    \"\"\"\n",
    "    # Positive samples: records where a wildfire event is recorded (Years != 0)\n",
    "    positive_samples = dataframe[dataframe['Years'] != 0]\n",
    "    # Negative samples: records with no wildfire event\n",
    "    negative_samples = dataframe[dataframe['Years'] == 0]\n",
    "    # Sample negatives to match the number of positives\n",
    "    negative_samples_sampled = negative_samples.sample(n=positive_samples.shape[0], random_state=1)\n",
    "    \n",
    "    # Split positive samples into those before and after the given year\n",
    "    after_year_samples = positive_samples[positive_samples['Years'] > year]\n",
    "    before_year_samples = positive_samples[positive_samples['Years'] < year]\n",
    "    \n",
    "    # Combine positive samples with a subsample of negatives for balance\n",
    "    after_year_balanced = pd.concat([\n",
    "        after_year_samples, \n",
    "        negative_samples_sampled.sample(n=after_year_samples.shape[0], random_state=1)\n",
    "    ])\n",
    "    before_year_balanced = pd.concat([\n",
    "        before_year_samples, \n",
    "        negative_samples_sampled.sample(n=before_year_samples.shape[0], random_state=1)\n",
    "    ])\n",
    "    \n",
    "    return after_year_balanced, before_year_balanced\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function: evaluate_models\n",
    "# -----------------------------------------------------------------------------\n",
    "def evaluate_models(model_dict, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models on the provided training and testing data.\n",
    "    \n",
    "    Parameters:\n",
    "      model_dict: Dictionary of models to evaluate.\n",
    "      X_train, y_train: Training data.\n",
    "      X_test, y_test: Testing data.\n",
    "      \n",
    "    Returns:\n",
    "      results: List of dictionaries containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for name, model in model_dict.items():\n",
    "        # Create a pipeline with scaling and the classifier\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        # Train the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        # If the classifier supports probability estimates, use them\n",
    "        if hasattr(pipeline.named_steps['classifier'], \"predict_proba\"):\n",
    "            y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_prob = y_pred\n",
    "        \n",
    "        # Compute and store metrics\n",
    "        results.append({\n",
    "            'model': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function: create_random_undersample\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_random_undersample(dataframe, train_size):\n",
    "    \"\"\"\n",
    "    Create balanced training and testing datasets via random undersampling.\n",
    "    \n",
    "    Parameters:\n",
    "      dataframe: Input DataFrame.\n",
    "      train_size: Proportion of the undersampled data to use for training.\n",
    "      \n",
    "    Returns:\n",
    "      X_train, X_test, y_train, y_test: Split balanced datasets.\n",
    "    \"\"\"\n",
    "    X = dataframe[factors]\n",
    "    y = dataframe['Wildfire']\n",
    "    rus = RandomUnderSampler(random_state=1)\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, train_size=train_size, random_state=1)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Temporal (Balanced) Sampling Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_samples = {}\n",
    "evaluation_results_years = []\n",
    "\n",
    "for year in split_years:\n",
    "    # Create balanced training (before year) and testing (after year) samples\n",
    "    train_set, test_set = create_balanced_sample(df, year)\n",
    "    balanced_samples[year] = {'train': train_set, 'test': test_set}\n",
    "    \n",
    "    X_train = train_set[factors]\n",
    "    y_train = train_set['Wildfire']\n",
    "    X_test = test_set[factors]\n",
    "    y_test = test_set['Wildfire']\n",
    "    \n",
    "    # Evaluate models on this split\n",
    "    year_results = evaluate_models(models, X_train, y_train, X_test, y_test)\n",
    "    # Add additional information to the results\n",
    "    for result in year_results:\n",
    "        result['year'] = year\n",
    "        result['method'] = 'Balanced Sampling'\n",
    "    evaluation_results_years.extend(year_results)\n",
    "\n",
    "evaluation_df_years = pd.DataFrame(evaluation_results_years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Random Undersampling Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sizes = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.90, 0.99]\n",
    "evaluation_results_test_sizes = []\n",
    "\n",
    "for train_size in test_sizes:\n",
    "    X_train, X_test, y_train, y_test = create_random_undersample(df, train_size)\n",
    "    test_size_results = evaluate_models(models, X_train, y_train, X_test, y_test)\n",
    "    for result in test_size_results:\n",
    "        result['test_size'] = train_size\n",
    "        result['method'] = 'Random Undersampling'\n",
    "    evaluation_results_test_sizes.extend(test_size_results)\n",
    "\n",
    "evaluation_df_test_sizes = pd.DataFrame(evaluation_results_test_sizes)\n",
    "\n",
    "# Combine both evaluation results for further analysis\n",
    "evaluation_df = pd.concat([evaluation_df_years, evaluation_df_test_sizes], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation with Timing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_with_timing(model_dict, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate models and record training and prediction times.\n",
    "    \n",
    "    Parameters:\n",
    "      model_dict: Dictionary of models.\n",
    "      X_train, y_train: Training data.\n",
    "      X_test, y_test: Testing data.\n",
    "      \n",
    "    Returns:\n",
    "      results: List of dictionaries with metrics and timing information.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for name, model in model_dict.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        # Time the model fitting\n",
    "        start_time = time.time()\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        fit_time = time.time() - start_time\n",
    "        \n",
    "        # Time prediction on training data\n",
    "        start_time = time.time()\n",
    "        y_train_pred = pipeline.predict(X_train)\n",
    "        if hasattr(pipeline.named_steps['classifier'], \"predict_proba\"):\n",
    "            y_train_prob = pipeline.predict_proba(X_train)[:, 1]\n",
    "        else:\n",
    "            y_train_prob = y_train_pred\n",
    "        train_pred_time = time.time() - start_time\n",
    "        \n",
    "        # Time prediction on testing data\n",
    "        start_time = time.time()\n",
    "        y_test_pred = pipeline.predict(X_test)\n",
    "        if hasattr(pipeline.named_steps['classifier'], \"predict_proba\"):\n",
    "            y_test_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_test_prob = y_test_pred\n",
    "        test_pred_time = time.time() - start_time\n",
    "        \n",
    "        # Record training set metrics\n",
    "        results.append({\n",
    "            'model': name,\n",
    "            'set': 'train',\n",
    "            'accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'roc_auc': roc_auc_score(y_train, y_train_prob),\n",
    "            'f1': f1_score(y_train, y_train_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "            'fit_time': fit_time,\n",
    "            'pred_time': train_pred_time\n",
    "        })\n",
    "        # Record testing set metrics\n",
    "        results.append({\n",
    "            'model': name,\n",
    "            'set': 'test',\n",
    "            'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_test_prob),\n",
    "            'f1': f1_score(y_test, y_test_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "            'fit_time': fit_time,\n",
    "            'pred_time': test_pred_time\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Evaluate with timing using temporal sampling\n",
    "evaluation_results_years_timing = []\n",
    "for year in split_years:\n",
    "    train_set, test_set = create_balanced_sample(df, year)\n",
    "    X_train = train_set[factors]\n",
    "    y_train = train_set['Wildfire']\n",
    "    X_test = test_set[factors]\n",
    "    y_test = test_set['Wildfire']\n",
    "    \n",
    "    year_results = evaluate_models_with_timing(models, X_train, y_train, X_test, y_test)\n",
    "    for result in year_results:\n",
    "        result['year'] = year\n",
    "        result['method'] = 'Balanced Sampling'\n",
    "    evaluation_results_years_timing.extend(year_results)\n",
    "\n",
    "evaluation_df_years_timing = pd.DataFrame(evaluation_results_years_timing)\n",
    "\n",
    "# Evaluate with timing using random undersampling\n",
    "evaluation_results_test_sizes_timing = []\n",
    "for train_size in test_sizes:\n",
    "    X_train, X_test, y_train, y_test = create_random_undersample(df, train_size)\n",
    "    test_size_results = evaluate_models_with_timing(models, X_train, y_train, X_test, y_test)\n",
    "    for result in test_size_results:\n",
    "        result['test_size'] = train_size\n",
    "        result['method'] = 'Random Undersampling'\n",
    "    evaluation_results_test_sizes_timing.extend(test_size_results)\n",
    "\n",
    "evaluation_df_test_sizes_timing = pd.DataFrame(evaluation_results_test_sizes_timing)\n",
    "evaluation_df_timing = pd.concat([evaluation_df_years_timing, evaluation_df_test_sizes_timing], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation of Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics to visualise\n",
    "metrics = ['accuracy', 'roc_auc', 'f1', 'rmse', 'fit_time', 'pred_time']\n",
    "# Define method labels for plotting\n",
    "methods = ['Temporal sampling', 'Random sampling']\n",
    "\n",
    "for metric in metrics:\n",
    "    # -------------------------------\n",
    "    # Plot for the training set\n",
    "    # -------------------------------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "    fig.suptitle(f'{metric.capitalize()} for Train Set')\n",
    "    \n",
    "    for ax, method in zip(axes, methods):\n",
    "        # Select the subset based on the method and training set\n",
    "        subset = evaluation_df_timing[(evaluation_df_timing['method'] == method) & (evaluation_df_timing['set'] == 'train')]\n",
    "        if method == 'Temporal sampling':\n",
    "            x_var = 'year'\n",
    "            xlabel = 'Year'\n",
    "            ax.set_xlim(2002, 2022)\n",
    "            ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "        else:\n",
    "            x_var = 'test_size'\n",
    "            xlabel = 'Train Size (%)'\n",
    "        \n",
    "        for model_name in subset['model'].unique():\n",
    "            model_subset = subset[subset['model'] == model_name]\n",
    "            ax.plot(model_subset[x_var], model_subset[metric],\n",
    "                    marker='o', label=model_name, linewidth=1, markersize=3)\n",
    "        \n",
    "        ax.set_title(method)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.savefig(f'evaluation_{metric}_train.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Plot for the testing set\n",
    "    # -------------------------------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "    fig.suptitle(f'{metric.capitalize()} for Test Set')\n",
    "    \n",
    "    for ax, method in zip(axes, methods):\n",
    "        subset = evaluation_df_timing[(evaluation_df_timing['method'] == method) & (evaluation_df_timing['set'] == 'test')]\n",
    "        if method == 'Temporal sampling':\n",
    "            x_var = 'year'\n",
    "            xlabel = 'Year'\n",
    "            ax.set_xlim(2002, 2022)\n",
    "            ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "        else:\n",
    "            x_var = 'test_size'\n",
    "            xlabel = 'Train Size (%)'\n",
    "        \n",
    "        for model_name in subset['model'].unique():\n",
    "            model_subset = subset[subset['model'] == model_name]\n",
    "            ax.plot(model_subset[x_var], model_subset[metric],\n",
    "                    marker='o', label=model_name, linewidth=1, markersize=3)\n",
    "        \n",
    "        ax.set_title(method)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.savefig(f'evaluation_{metric}_test.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Analysis: Histogram Visualisation of Wildfire Susceptibility\n",
    "\n",
    "This section creates balanced samples for a specific year and plots histograms for selected models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create balanced samples for a specific analysis year (e.g. 2012)\n",
    "year_for_analysis = 2012\n",
    "after_year_balanced, before_year_balanced = create_balanced_sample(df, year_for_analysis)\n",
    "\n",
    "# Prepare training and testing data using the balanced sample\n",
    "X_train = before_year_balanced[factors]\n",
    "y_train = before_year_balanced['Wildfire']\n",
    "X_test = after_year_balanced[factors]\n",
    "y_test = after_year_balanced['Wildfire']\n",
    "\n",
    "# Define a dictionary of models for further analysis (parameters can be adjusted)\n",
    "models_analysis = {\n",
    "    'RF': RandomForestClassifier(bootstrap=False, max_depth=9, n_estimators=25, random_state=1),\n",
    "    'HGB': HistGradientBoostingClassifier(min_samples_leaf=190, max_iter=90, max_depth=7, max_leaf_nodes=33, learning_rate=0.15),\n",
    "    'MLP': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', MLPClassifier(activation='tanh', hidden_layer_sizes=(12,), max_iter=300, random_state=1))\n",
    "    ]),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=34, p=2),\n",
    "    'LR': LogisticRegression(random_state=1),\n",
    "    'SVM': SVC(C=100, random_state=1, probability=True)\n",
    "}\n",
    "\n",
    "# Train each model and add the predicted probabilities to the dataframe\n",
    "for name, model in models_analysis.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # Predict on the entire dataframe using the selected factors\n",
    "    prob = pipeline.predict_proba(df[factors])[:, 1]\n",
    "    df[name] = prob\n",
    "\n",
    "# Plot histograms of the wildfire susceptibility index for selected models.\n",
    "# (Note: Replace 'tomporal' with your actual dataframe name if necessary.)\n",
    "selected_models = ['RF', 'HGB', 'MLP', 'KNN', 'LR']\n",
    "for model_name in selected_models:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(\n",
    "        data=df.query('Years > 2012 and Years != 2018'),\n",
    "        x=model_name,\n",
    "        hue='Years',\n",
    "        common_norm=True,\n",
    "        palette='jet_r',\n",
    "        stat='density',\n",
    "        multiple='stack',\n",
    "        bins=34\n",
    "    )\n",
    "    plt.xlabel('Wildfire Susceptibility Index', size=12)\n",
    "    plt.ylabel('Wildfire Density', size=12)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_histo_temporal_scaled_common.png', dpi=500)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Visualisation: Bar Plots of Evaluation Metrics\n",
    "\n",
    "This final section replaces some method names for clarity, calculates processing time, and creates bar plots for selected evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace method names for clarity\n",
    "evaluation_df_timing['method'] = evaluation_df_timing['method'].str.replace('Random Undersampling', 'Random sampling')\n",
    "evaluation_df_timing['method'] = evaluation_df_timing['method'].str.replace('Balanced Sampling', 'Temporal sampling')\n",
    "\n",
    "# Calculate processing time as the sum of fit and prediction times\n",
    "evaluation_df_timing['Processing time (s)'] = evaluation_df_timing['fit_time'] + evaluation_df_timing['pred_time']\n",
    "\n",
    "# Filter results for the test set only\n",
    "eval_df_test = evaluation_df_timing[evaluation_df_timing['set'] == 'test']\n",
    "\n",
    "# Melt the DataFrame for easier plotting of metrics\n",
    "df_melted = eval_df_test.melt(\n",
    "    id_vars=['model', 'method'],\n",
    "    value_vars=['accuracy', 'roc_auc', 'f1', 'rmse', 'Processing time (s)'],\n",
    "    var_name='metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "# Filter for selected models\n",
    "df_melted = df_melted.query('model in [\"RF\", \"HGB\", \"MLP\", \"KNN\", \"LR\"]')\n",
    "\n",
    "# Create subplots for each metric\n",
    "metrics_to_plot = ['accuracy', 'roc_auc', 'f1', 'rmse', 'Processing time (s)']\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define pastel colours for the bars\n",
    "colors = [\"#f6bc66\", \"#f49097\"]\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    sns.barplot(\n",
    "        data=df_melted[df_melted['metric'] == metric],\n",
    "        x='model', y='value', hue='method', ax=axes[i],\n",
    "        palette=colors, linewidth=1, edgecolor=\".4\"\n",
    "    )\n",
    "    axes[i].set_xlabel('Model')\n",
    "    axes[i].set_ylabel(metric.capitalize())\n",
    "    axes[i].legend(title='Sampling', loc='lower left')\n",
    "    # For some metrics, adjust the y-axis limits if required (example shown for the first row)\n",
    "    if i < 3:\n",
    "        axes[i].set_ylim(0.5, 1)\n",
    "\n",
    "# Adjust one of the legends if necessary\n",
    "axes[4].legend(title='Sampling', loc='upper left')\n",
    "\n",
    "# Remove any unused subplot\n",
    "if len(metrics_to_plot) < len(axes):\n",
    "    for j in range(len(metrics_to_plot), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "plt.savefig('CV_Metrics.png', dpi=500)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
